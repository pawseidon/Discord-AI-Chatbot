import re
import asyncio
import logging
import json
from typing import Dict, Any, List, Tuple, Optional, Union
from bot_utilities.sequential_thinking import create_sequential_thinking, SequentialThinking
from bot_utilities.react_utils import run_react_agent
from bot_utilities.chain_of_verification import run_verification
from bot_utilities.speculative_rag import SpeculativeRAG
from bot_utilities.reflective_rag import SelfReflectiveRAG
from bot_utilities.ai_utils import get_ai_provider, generate_response, should_use_sequential_thinking
from bot_utilities.token_utils import token_optimizer

# Set up logging
logger = logging.getLogger("reasoning_router")
logger.setLevel(logging.INFO)

class ResponseCandidate:
    """A response candidate generated by a specific reasoning method"""
    def __init__(self, 
                 response: str,
                 method: str,
                 reasoning_process: str = "",
                 confidence_score: float = 0.0,
                 metadata: Dict[str, Any] = None):
        self.response = response
        self.method = method
        self.reasoning_process = reasoning_process
        self.confidence_score = confidence_score
        self.metadata = metadata or {}
        
    def __str__(self):
        return f"ResponseCandidate(method={self.method}, confidence={self.confidence_score:.2f})"

class ResponseEvaluator:
    """
    Evaluates response candidates and selects the best one based on 
    quality metrics, coherence, and contextual relevance
    """
    def __init__(self, llm_provider=None):
        self.llm_provider = llm_provider
        self.evaluation_criteria = {
            "relevance": 0.35,    # How relevant the response is to the query
            "coherence": 0.25,    # How coherent and well-structured the response is
            "accuracy": 0.20,     # How factually accurate the response is 
            "completeness": 0.15, # How completely the response addresses the query
            "conciseness": 0.05   # How concise and to-the-point the response is
        }
    
    async def evaluate_response(self, candidate: ResponseCandidate, query: str, context: List[Dict[str, str]] = None) -> float:
        """
        Evaluate a response candidate against criteria and assign a quality score
        
        Args:
            candidate: The response candidate
            query: The original user query
            context: Optional conversation context
            
        Returns:
            float: Quality score between 0.0 and 1.0
        """
        # Method-specific baseline scores
        method_baseline_scores = {
            "sequential": 0.85,  # Sequential thinking often produces well-structured responses
            "react": 0.82,       # ReAct is good for tool-based tasks
            "verification": 0.87, # Chain-of-Verification is strong for factual accuracy
            "speculative_rag": 0.80, # Speculative RAG is good for information retrieval
            "reflective_rag": 0.83, # Reflective RAG is good for contextual reasoning
            "default": 0.75      # Default method has a lower baseline
        }
        
        # Start with the method's baseline score
        method = candidate.method
        base_score = method_baseline_scores.get(method, 0.75)
        
        # Adjust score based on response content
        adjusted_score = base_score
        
        # Check for signs of high-quality responses
        if "**" in candidate.response and ("Step" in candidate.response or "Thought" in candidate.response):
            # Well-structured sequential thinking with clear steps
            adjusted_score += 0.05
        
        if any(marker in candidate.response.lower() for marker in ["first", "second", "third", "finally", "conclusion"]):
            # Response has a clear structure
            adjusted_score += 0.03
        
        # Check for relevance by looking for query keywords in the response
        query_keywords = set(query.lower().split())
        response_words = set(candidate.response.lower().split())
        keyword_overlap = len(query_keywords.intersection(response_words)) / len(query_keywords) if query_keywords else 0
        
        relevance_boost = min(0.07, keyword_overlap * 0.1)
        adjusted_score += relevance_boost
        
        # Length checks - penalize extremely short or verbose responses
        word_count = len(candidate.response.split())
        if word_count < 30:
            # Too short, likely incomplete
            adjusted_score -= 0.15
        elif word_count > 1000:
            # Too verbose, might be unfocused
            adjusted_score -= 0.05
        
        # Use AI evaluation for more sophisticated assessment if provider available
        if self.llm_provider:
            try:
                ai_evaluation = await self.evaluate_with_llm(candidate.response, query, context)
                # Blend AI evaluation with rule-based score
                adjusted_score = 0.7 * adjusted_score + 0.3 * ai_evaluation
            except Exception as e:
                logger.error(f"Error in AI evaluation: {e}")
                # Continue with rule-based score if AI evaluation fails
        
        # Ensure score is in valid range [0.0, 1.0]
        final_score = max(0.0, min(1.0, adjusted_score))
        return final_score
    
    async def evaluate_with_llm(self, response: str, query: str, context: List[Dict[str, str]] = None) -> float:
        """Use the LLM to evaluate response quality"""
        if not self.llm_provider:
            return 0.8  # Default score if no provider
        
        try:
            eval_prompt = f"""Evaluate this response to the user's query. Score from 0.0 to 1.0 based on:
- Relevance to query (35%)
- Coherence and structure (25%)
- Factual accuracy (20%)
- Completeness (15%)
- Conciseness (5%)

USER QUERY: {query}

RESPONSE TO EVALUATE:
{response}

EVALUATION:
Think step by step about each criterion, then provide a final score between 0.0 and 1.0.
Format your final answer as a JSON object with a single "score" field: {{"score": X.X}}
"""
            # Get evaluation from the LLM
            result = await self.llm_provider.async_call(
                prompt=eval_prompt,
                temperature=0.1,
                max_tokens=500
            )
            
            # Extract score from response
            try:
                # Try to parse as JSON
                score_match = re.search(r'{\s*"score"\s*:\s*(\d+\.\d+|\d+)\s*}', result)
                if score_match:
                    return float(score_match.group(1))
                
                # Fallback: look for a floating-point number
                number_match = re.search(r'(\d+\.\d+|\d+)\s*/\s*1\.0|(\d+\.\d+|\d+)', result)
                if number_match:
                    # Get the first group that matched
                    score_str = next(group for group in number_match.groups() if group is not None)
                    return float(score_str)
                
                # If parsing fails, return a default score
                return 0.8
            except Exception as e:
                logger.error(f"Error parsing evaluation result: {e}")
                return 0.8
                
        except Exception as e:
            logger.error(f"Error in LLM evaluation: {e}")
            return 0.8
    
    async def select_best_response(self, candidates: List[ResponseCandidate], query: str, context: List[Dict[str, str]] = None) -> ResponseCandidate:
        """
        Evaluate all candidates and select the best response
        
        Args:
            candidates: List of response candidates
            query: The original user query
            context: Optional conversation context
            
        Returns:
            ResponseCandidate: The best response candidate
        """
        if not candidates:
            # No candidates to evaluate
            return None
        
        if len(candidates) == 1:
            # Only one candidate, no need to evaluate
            return candidates[0]
        
        # Evaluate all candidates
        for candidate in candidates:
            score = await self.evaluate_response(candidate, query, context)
            candidate.confidence_score = score
            logger.info(f"Evaluated {candidate.method} response: score {score:.2f}")
        
        # Select the best candidate based on score
        best_candidate = max(candidates, key=lambda c: c.confidence_score)
        
        # Log selection
        methods = [f"{c.method}({c.confidence_score:.2f})" for c in candidates]
        logger.info(f"Selected {best_candidate.method} from {methods}")
        
        return best_candidate

class QueryAnalysis:
    """Analysis results for a query to determine appropriate reasoning mechanism"""
    def __init__(self, 
                 query: str, 
                 is_factual: bool = False,
                 is_complex: bool = False,
                 needs_verification: bool = False,
                 needs_tools: bool = False,
                 needs_search: bool = False,
                 reasoning_score: float = 0.0,
                 recommended_method: str = "default",
                 reasoning: str = ""):
        self.query = query
        self.is_factual = is_factual
        self.is_complex = is_complex
        self.needs_verification = needs_verification
        self.needs_tools = needs_tools
        self.needs_search = needs_search
        self.reasoning_score = reasoning_score
        self.recommended_method = recommended_method
        self.reasoning = reasoning

class ReasoningRouter:
    """
    Dynamic router that selects the most appropriate reasoning method based on query analysis
    """
    def __init__(self, 
                 llm_provider=None, 
                 use_sequential_thinking=True,
                 use_react=True,
                 use_verification=True,
                 use_speculative_rag=True,
                 use_reflective_rag=True,
                 use_response_evaluation=True):
        self.llm_provider = llm_provider
        self.sequential_thinking = create_sequential_thinking(llm_provider) if use_sequential_thinking else None
        self.use_react = use_react
        self.use_verification = use_verification
        self.use_speculative_rag = use_speculative_rag
        self.use_reflective_rag = use_reflective_rag
        self.use_response_evaluation = use_response_evaluation
        self.response_evaluator = ResponseEvaluator(llm_provider) if use_response_evaluation else None
        
        # Thresholds for different reasoning methods
        self.sequential_threshold = 0.8  # Match the threshold in should_use_sequential_thinking
        self.react_threshold = 0.7
        self.verification_threshold = 0.6
        self.rag_threshold = 0.5
        
        # Log initialization status
        logger.info(f"ReasoningRouter initialized with capabilities: "
                   f"Sequential={use_sequential_thinking}, "
                   f"ReAct={use_react}, "
                   f"Verification={use_verification}, "
                   f"SpecRAG={use_speculative_rag}, "
                   f"ReflectRAG={use_reflective_rag}, "
                   f"ResponseEval={use_response_evaluation}")
    
    async def analyze_query(self, query: str) -> QueryAnalysis:
        """
        Analyze the query to determine which reasoning mechanism is most appropriate
        
        Args:
            query: The user's query string
            
        Returns:
            QueryAnalysis: Analysis results
        """
        logger.info(f"Analyzing query: {query[:50]}...")
        
        # Initialize the analysis with default values
        analysis = QueryAnalysis(query=query)
        
        # Check if query is factual (likely to benefit from verification)
        factual_patterns = [
            r'\b(who|what|when|where|why|how|which)\b', # Question words
            r'\b(true|false|correct|incorrect|accurate|inaccurate|fact|truth)\b', # Truth statements
            r'\b(in|during|at|on)\s\d{4}\b', # Years
            r'\b(history|historical|scientist|inventor|discover|invent|create|develop)\b', # Historical topics
            r'\b(population|statistics|percentage|number|count|total)\b', # Statistical queries
            r'\b(define|definition|explain|describe|what is|what are)\b' # Definition queries
        ]
        
        factual_score = sum(1 for pattern in factual_patterns if re.search(pattern, query.lower()))
        analysis.is_factual = factual_score >= 2
        
        # Check if query would benefit from tool use
        tool_patterns = [
            r'\b(search|find|look up|google|online|internet|web|recent|latest|current|news)\b', # Search requests
            r'\b(calculate|compute|convert|translate|transform)\b', # Computation requests
            r'\b(weather|temperature|forecast)\b', # Weather queries
            r'\b(crypto|bitcoin|ethereum|price|stock|market)\b', # Financial queries
            r'\b(address|location|directions|map|distance|how far)\b', # Location queries
            r'\b(code|program|function|algorithm|implement)\b' # Coding requests
        ]
        
        tool_score = sum(1 for pattern in tool_patterns if re.search(pattern, query.lower()))
        analysis.needs_tools = tool_score >= 1
        
        # Check if query specifically requests search
        search_patterns = [
            r'\b(search|find|look up|google|online|internet|web)\b',
            r'\b(recent|latest|current|news|today|yesterday|this week|this month|this year)\b',
            r'\b(what happened|what is going on|what is happening)\b'
        ]
        
        search_score = sum(1 for pattern in search_patterns if re.search(pattern, query.lower()))
        analysis.needs_search = search_score >= 1
        
        # Check for complexity using the existing function
        use_sequential, complexity_score, reasoning = await should_use_sequential_thinking(query)
        analysis.is_complex = use_sequential
        analysis.reasoning_score = complexity_score
        
        # Determine if verification would be beneficial
        verification_indicators = [
            r'\b(verify|check|confirm|validate|ensure|accurate|accuracy|fact check|correct)\b',
            r'\b(true|false|truth|lie|misleading|misinformation|disinformation)\b',
            r'\b(source|citation|reference|evidence|proof|study|research)\b'
        ]
        
        verification_score = sum(0.5 for pattern in verification_indicators if re.search(pattern, query.lower()))
        if analysis.is_factual:
            verification_score += 1.0
            
        analysis.needs_verification = verification_score >= 0.5
        
        # Determine the most appropriate method based on the analysis
        # Start with sequential thinking for complex queries
        if analysis.is_complex and complexity_score >= self.sequential_threshold and self.sequential_thinking:
            analysis.recommended_method = "sequential"
            analysis.reasoning = f"Complex query detected. {reasoning}"
        
        # For queries needing tools or factual checking with immediate actions
        elif analysis.needs_tools and self.use_react and tool_score >= 1:
            analysis.recommended_method = "react"
            analysis.reasoning = f"Tool use beneficial for this query. Tool indicators: {tool_score}"
        
        # For factual queries that need verification but not complex reasoning
        elif analysis.is_factual and analysis.needs_verification and self.use_verification:
            analysis.recommended_method = "verification"
            analysis.reasoning = f"Factual query needing verification. Factual score: {factual_score}, Verification score: {verification_score:.1f}"
        
        # For queries that need search or recent information
        elif analysis.needs_search and self.use_speculative_rag:
            analysis.recommended_method = "speculative_rag"
            analysis.reasoning = f"Query likely needs recent information. Search score: {search_score}"
        
        # For general queries that don't fit specific patterns but still need depth
        elif analysis.reasoning_score >= self.rag_threshold and self.use_reflective_rag:
            analysis.recommended_method = "reflective_rag"
            analysis.reasoning = f"Query would benefit from deep contextual reasoning. Score: {analysis.reasoning_score:.1f}"
        
        # Default fallback
        else:
            analysis.recommended_method = "default"
            analysis.reasoning = f"Standard processing sufficient. Complexity: {analysis.reasoning_score:.1f}"
        
        logger.info(f"Analysis result: {analysis.recommended_method} ({analysis.reasoning})")
        return analysis
    
    async def generate_response_candidate(self, method: str, query: str, username: str = None, user_id: str = None, 
                                  history: List[Dict[str, str]] = None, enhanced_instructions: str = None) -> ResponseCandidate:
        """
        Generate a response using a specific reasoning method
        
        Args:
            method: Reasoning method to use
            query: User query
            username: User's name
            user_id: User's ID 
            history: Conversation history
            enhanced_instructions: Custom instructions
            
        Returns:
            ResponseCandidate: The generated response with metadata
        """
        try:
            response = None
            metadata = {"method": method}
            
            # Sequential thinking for complex problems
            if method == "sequential":
                logger.info("Generating response with sequential thinking")
                if not self.sequential_thinking:
                    self.sequential_thinking = create_sequential_thinking(self.llm_provider)
                
                response = await self.sequential_thinking.generate(
                    prompt=query,
                    username=username,
                    conversation_history=history,
                    enhanced_instructions=enhanced_instructions
                )
                
                # Add reasoning emoji to indicate sequential thinking was used
                metadata["emoji"] = "üîÑ"
                
            # ReAct pattern for tool-based reasoning
            elif method == "react" and self.use_react:
                logger.info("Generating response with ReAct agent")
                response = await run_react_agent(
                    query=query, 
                    username=username,
                    user_id=user_id,
                    history=history,
                    instructions=enhanced_instructions
                )
                
                # Add tools emoji
                metadata["emoji"] = "üîç"
                
            # Chain of Verification for factual claims
            elif method == "verification" and self.use_verification:
                logger.info("Generating response with Chain of Verification")
                response = await run_verification(
                    query=query,
                    username=username,
                    history=history,
                    instructions=enhanced_instructions
                )
                
                # Add verification emoji
                metadata["emoji"] = "‚úÖ"
                
            # Speculative RAG for search-intensive queries
            elif method == "speculative_rag" and self.use_speculative_rag:
                logger.info("Generating response with Speculative RAG")
                spec_rag = SpeculativeRAG(llm_provider=self.llm_provider)
                response = await spec_rag.generate(
                    query=query,
                    username=username,
                    history=history,
                    instructions=enhanced_instructions
                )
                
                # Add research emoji
                metadata["emoji"] = "üìö"
                
            # Self-Reflective RAG for deep contextual reasoning
            elif method == "reflective_rag" and self.use_reflective_rag:
                logger.info("Generating response with Self-Reflective RAG")
                reflective_rag = SelfReflectiveRAG(llm_provider=self.llm_provider)
                response = await reflective_rag.generate(
                    query=query,
                    username=username,
                    history=history,
                    instructions=enhanced_instructions
                )
                
                # Add brain emoji
                metadata["emoji"] = "üß†"
                
            # Default processing for simple queries
            else:
                logger.info("Generating response with default processing")
                formatted_history = []
                if history:
                    for entry in history:
                        formatted_history.append(entry)
                
                # Add the current query
                formatted_history.append({"role": "user", "content": query})
                
                # Generate a response
                response = await generate_response(
                    instructions=enhanced_instructions or "", 
                    history=formatted_history
                )
                
                # Add standard emoji
                metadata["emoji"] = "üí¨"
            
            # Create and return the response candidate
            return ResponseCandidate(
                response=response,
                method=method,
                reasoning_process="",  # Could store reasoning process if available
                confidence_score=0.0,  # Will be set by evaluator
                metadata=metadata
            )
            
        except Exception as e:
            logger.error(f"Error generating response with method {method}: {str(e)}")
            # Return None on error, will be filtered out later
            return None
    
    async def process(self, query: str, username: str = None, user_id: str = None, history: List[Dict[str, str]] = None, enhanced_instructions: str = None) -> Tuple[str, Dict[str, Any]]:
        """
        Process a query using the most appropriate reasoning method
        
        Args:
            query: The user's query
            username: User's name for personalization
            user_id: User's ID for tracking
            history: Conversation history
            enhanced_instructions: Custom instructions for the AI
            
        Returns:
            Tuple[str, Dict]: The response and metadata about the processing
        """
        if not query:
            return "I couldn't understand your query. Could you please rephrase it?", {"method": "error"}
        
        logger.info(f"Processing query from {username or 'unknown user'}: {query[:50]}...")
        
        # Initialize the LLM provider if it hasn't been done yet
        if not self.llm_provider:
            try:
                self.llm_provider = await get_ai_provider()
                if self.response_evaluator:
                    self.response_evaluator.llm_provider = self.llm_provider
            except Exception as e:
                logger.error(f"Failed to initialize LLM provider: {e}")
                return "I'm having trouble connecting to my language processing capabilities. Please try again later.", {"method": "error"}
        
        # Analyze the query to determine the best processing method
        analysis = await self.analyze_query(query)
        
        # Use the recommended method based on analysis
        recommended_method = analysis.recommended_method
        
        # For simple evaluation flow (no multi-candidate evaluation)
        if not self.use_response_evaluation:
            try:
                candidate = await self.generate_response_candidate(
                    method=recommended_method,
                    query=query,
                    username=username,
                    user_id=user_id,
                    history=history,
                    enhanced_instructions=enhanced_instructions
                )
                
                if candidate:
                    return candidate.response, {
                        "method": candidate.method,
                        "reasoning": analysis.reasoning,
                        "complexity_score": analysis.reasoning_score,
                        "emoji": candidate.metadata.get("emoji", "üí¨"),
                        "is_factual": analysis.is_factual,
                        "needs_tools": analysis.needs_tools,
                        "needs_verification": analysis.needs_verification
                    }
                else:
                    # Fallback to default if the recommended method fails
                    logger.warning(f"Recommended method {recommended_method} failed, falling back to default")
                    default_candidate = await self.generate_response_candidate(
                        method="default",
                        query=query,
                        username=username,
                        user_id=user_id,
                        history=history,
                        enhanced_instructions=enhanced_instructions
                    )
                    
                    if default_candidate:
                        return default_candidate.response, {
                            "method": "default",
                            "reasoning": "Fallback after method failure",
                            "complexity_score": analysis.reasoning_score,
                            "emoji": "‚ö†Ô∏è",
                            "is_fallback": True
                        }
                    else:
                        return "I'm experiencing technical difficulties processing your request. Please try again.", {"method": "error"}
            
            except Exception as e:
                logger.error(f"Error in reasoning router: {str(e)}")
                # Fall back to standard processing on error
                try:
                    # Try with default processing
                    formatted_history = []
                    if history:
                        for entry in history:
                            formatted_history.append(entry)
                    
                    formatted_history.append({"role": "user", "content": query})
                    
                    response = await generate_response(
                        instructions=enhanced_instructions or "",
                        history=formatted_history
                    )
                    
                    return response, {"method": "fallback", "error": str(e), "emoji": "‚ö†Ô∏è"}
                except Exception as fallback_error:
                    logger.error(f"Fallback also failed: {str(fallback_error)}")
                    return "I'm having trouble processing your request right now. Please try again later.", {"method": "error"}
        
        # For complex evaluation flow with multiple candidates
        else:
            try:
                # Step 1: Determine which methods to try based on the query analysis
                methods_to_try = ["default"]  # Always include default as a fallback
                
                # Add the recommended method from analysis
                if recommended_method != "default":
                    methods_to_try.insert(0, recommended_method)
                
                # Add secondary methods based on query characteristics
                if analysis.is_complex and "sequential" not in methods_to_try and self.sequential_thinking:
                    methods_to_try.append("sequential")
                    
                if analysis.needs_verification and "verification" not in methods_to_try and self.use_verification:
                    methods_to_try.append("verification")
                    
                if analysis.needs_tools and "react" not in methods_to_try and self.use_react:
                    methods_to_try.append("react")
                    
                if analysis.needs_search and "speculative_rag" not in methods_to_try and self.use_speculative_rag:
                    methods_to_try.append("speculative_rag")
                
                # Limit to max 3 methods to avoid excessive resource usage
                methods_to_try = methods_to_try[:3]
                
                logger.info(f"Trying methods: {methods_to_try}")
                
                # Step 2: Generate response candidates in parallel
                candidate_tasks = [
                    self.generate_response_candidate(
                        method=method,
                        query=query,
                        username=username,
                        user_id=user_id,
                        history=history,
                        enhanced_instructions=enhanced_instructions
                    )
                    for method in methods_to_try
                ]
                
                candidates = await asyncio.gather(*candidate_tasks)
                
                # Filter out None responses (failed methods)
                valid_candidates = [c for c in candidates if c is not None]
                
                if not valid_candidates:
                    # If all methods failed, return an error message
                    logger.error("All response generation methods failed")
                    return "I'm experiencing technical difficulties. Please try again later.", {"method": "error"}
                
                # Step 3: Evaluate candidates and select the best one
                best_candidate = await self.response_evaluator.select_best_response(
                    candidates=valid_candidates,
                    query=query,
                    context=history
                )
                
                # Step 4: Return the best response with enhanced metadata
                response_metadata = {
                    "method": best_candidate.method,
                    "reasoning": analysis.reasoning,
                    "complexity_score": analysis.reasoning_score,
                    "confidence_score": best_candidate.confidence_score,
                    "emoji": best_candidate.metadata.get("emoji", "üí¨"),
                    "candidates_evaluated": len(valid_candidates),
                    "is_factual": analysis.is_factual,
                    "needs_tools": analysis.needs_tools,
                    "needs_verification": analysis.needs_verification
                }
                
                # If multiple methods were tried, add that to metadata
                if len(valid_candidates) > 1:
                    response_metadata["alternative_methods"] = [
                        {"method": c.method, "score": c.confidence_score}
                        for c in valid_candidates if c.method != best_candidate.method
                    ]
                
                logger.info(f"Selected response from method: {best_candidate.method} with score: {best_candidate.confidence_score:.2f}")
                return best_candidate.response, response_metadata
                
            except Exception as e:
                logger.error(f"Error in complex evaluation: {str(e)}")
                # Fall back to standard processing on error
                try:
                    # Try with default processing
                    formatted_history = []
                    if history:
                        for entry in history:
                            formatted_history.append(entry)
                    
                    formatted_history.append({"role": "user", "content": query})
                    
                    response = await generate_response(
                        instructions=enhanced_instructions or "",
                        history=formatted_history
                    )
                    
                    return response, {"method": "fallback", "error": str(e), "emoji": "‚ö†Ô∏è"}
                except Exception as fallback_error:
                    logger.error(f"Fallback also failed: {str(fallback_error)}")
                    return "I'm having trouble processing your request right now. Please try again later.", {"method": "error"}

def create_reasoning_router(llm_provider=None):
    """Create and return a reasoning router instance"""
    return ReasoningRouter(llm_provider=llm_provider) 